@article{10.14778/3681954.3681959,
author = {Chen, Meng and Zhang, Kai and He, Zhenying and Jing, Yinan and Wang, X. Sean},
title = {RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681959},
doi = {10.14778/3681954.3681959},
abstract = {Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical component in many applications, including recommendation systems and large language model-based applications. With the advancement of multimodal neural models, which transform data from different modalities into a shared high-dimensional space as feature vectors, cross-modal ANNS aims to use the data vector from one modality (e.g., texts) as the query to retrieve the most similar items from another (e.g., images or videos). However, there is an inherent distribution gap between embeddings from different modalities, and cross-modal queries become Out-of-Distribution (OOD) to the base data. Consequently, state-of-the-art ANNS approaches suffer poor performance for OOD workloads.In this paper, we quantitatively analyze the properties of the OOD workloads to gain an understanding of their ANNS efficiency. Unlike single-modal workloads, we reveal OOD queries spatially deviate from base data, and the k-nearest neighbors of an OOD query are distant from each other in the embedding space. The property breaks the assumptions of existing ANNS approaches and mismatches their design for efficient search. With the insights from the OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an efficient ANNS graph index that is built under the guidance of query distribution. Extensive experiments show that RoarGraph significantly outperforms state-of-the-art approaches on modern cross-modal datasets, achieving up to 3.56\texttimes{} faster search speed at a 90\% recall rate for OOD queries.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2735â€“2749},
numpages = {15},
award = {Huawei OlympusMons Awards (Pioneer)},
selected= {true}
}

@article{liu2024retrievalattention,
  title={RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval},
  author={Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and others},
  journal={arXiv preprint arXiv:2409.10516; accepted by NeurIPS 2024 ENLSP Workshop; to appear in NeurIPS 2025 Main Track},
  abstract={Transformer-based Large Language Models (LLMs) have become increasingly important. However, scaling LLMs to longer contexts incurs slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper presents RetrievalAttention, a training-free approach to both accelerate the decoding phase and reduce GPU memory consumption by pre-building KV vector indexes for fixed contexts and maintaining them in CPU memory for efficient retrieval. Unlike conventional KV cache methods, RetrievalAttention integrate approximate nearest neighbor search (ANNS) indexes into attention computation. We observe that off-the-shelf ANNS techniques often fail due to the out-of-distribution (OOD) nature of query and key vectors in attention mechanisms. RetrievalAttention overcomes this with an attention-aware vector index. Our evaluation shows RetrievalAttention achieves near full attention accuracy while accessing only 1-3% of the data, significantly reducing inference costs. Remarkably, RetrievalAttention enables LLMs with 8B parameters to handle 128K tokens on a single NVIDIA RTX4090 (24GB), achieving a decoding speed of 0.107 seconds per token.},
  url={https://huggingface.co/papers/2409.10516},
  year={2024},
  award={Best Paper in ENLSP},
  selected= {true}
}

@article{simhadri2024results,
  title={Results of the Big ANN: NeurIPS'23 competition},
  journal={arXiv preprint arXiv:2409.17424; to appear in NeurIPS 2025 Dataset & Benchmark Track},
  year={2024},
  url={https://arxiv.org/abs/2409.17424},
  selected= {true}
}